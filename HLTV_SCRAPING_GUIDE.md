# ğŸŒ HLTV'DEN GERÃ‡EK VERÄ° Ã‡EKME REHBERÄ°

## ğŸ“‹ Ä°Ã‡Ä°NDEKÄ°LER
1. [Gereksinimler](#gereksinimler)
2. [Kurulum](#kurulum)
3. [Temel KullanÄ±m](#temel-kullanÄ±m)
4. [GeliÅŸmiÅŸ Ã–zellikler](#geliÅŸmiÅŸ-Ã¶zellikler)
5. [Sorun Giderme](#sorun-giderme)
6. [Etik ve Yasal](#etik-ve-yasal)

---

## ğŸ”§ GEREKSÄ°NÄ°MLER

### YazÄ±lÄ±m Gereksinimleri

1. **Python 3.8+**
   ```bash
   python --version
   # Python 3.8 veya Ã¼zeri
   ```

2. **Chrome veya Chromium**
   ```bash
   # Chrome yÃ¼klÃ¼ mÃ¼ kontrol et
   google-chrome --version  # Linux
   # veya
   "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome" --version  # Mac
   ```

3. **Python Paketleri**
   ```bash
   pip install selenium webdriver-manager pandas
   ```

### Ä°nternet BaÄŸlantÄ±sÄ±
- âœ… Stabil internet baÄŸlantÄ±sÄ± gerekli
- âœ… HLTV.org'a eriÅŸim gerekli
- âš ï¸  VPN kullanÄ±yorsanÄ±z, bazÄ± Ã¼lkelerden eriÅŸim kÄ±sÄ±tlÄ± olabilir

---

## ğŸ’» KURULUM

### AdÄ±m 1: Paketleri YÃ¼kle

```bash
pip install selenium==4.15.2
pip install webdriver-manager==4.0.1
pip install pandas==2.1.3
```

### AdÄ±m 2: Chrome YÃ¼kle (EÄŸer yoksa)

**Ubuntu/Debian:**
```bash
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo dpkg -i google-chrome-stable_current_amd64.deb
sudo apt-get install -f
```

**Mac:**
```bash
brew install --cask google-chrome
```

**Windows:**
- https://www.google.com/chrome/ adresinden indirin

### AdÄ±m 3: Scraper'Ä± Test Et

```bash
python real_hltv_scraper.py
```

---

## ğŸš€ TEMEL KULLANIM

### Basit Scraping

```python
from real_hltv_scraper import RealHLTVScraper

# Scraper oluÅŸtur
scraper = RealHLTVScraper(headless=True)

# Driver baÅŸlat
scraper.setup_driver()

try:
    # GeÃ§miÅŸ maÃ§larÄ± Ã§ek (3 sayfa = ~150 maÃ§)
    results = scraper.scrape_results(num_pages=3)
    results.to_csv('results.csv', index=False)
    
    # Gelecek maÃ§larÄ± Ã§ek
    upcoming = scraper.scrape_upcoming_matches()
    upcoming.to_csv('upcoming.csv', index=False)
    
finally:
    scraper.close()
```

### Komut SatÄ±rÄ±ndan

```bash
# Basit kullanÄ±m
python real_hltv_scraper.py

# TarayÄ±cÄ±yÄ± gÃ¶rerek (debugging)
# Script iÃ§inde headless=False yapÄ±n
```

---

## ğŸ”¥ GELÄ°ÅMÄ°Å Ã–ZELLÄ°KLER

### 1. Daha Fazla Sayfa Ã‡ekme

```python
# 10 sayfa = ~500 maÃ§
results = scraper.scrape_results(num_pages=10)
```

âš ï¸  **Dikkat**: Ã‡ok fazla sayfa Ã§ekerken:
- Rate limiting olabilir
- IP ban riski artar
- Ä°ÅŸlem sÃ¼resi uzar (sayfa baÅŸÄ± ~5 saniye)

### 2. MaÃ§ DetaylarÄ±nÄ± Ã‡ekme

```python
# Belirli bir maÃ§Ä±n detayÄ±nÄ± Ã§ek
match_url = "https://www.hltv.org/matches/2369161/liquid-vs-nip-iem-katowice-2026"
details = scraper.scrape_match_details(match_url)

print(details['maps'])  # Harita bazlÄ± sonuÃ§lar
```

### 3. Otomatik GÃ¼nlÃ¼k Scraping

**daily_scraper.py** oluÅŸturun:

```python
"""
GÃ¼nlÃ¼k otomatik HLTV scraper
Her gÃ¼n belirlenen saatte Ã§alÄ±ÅŸÄ±r
"""

import schedule
import time
from real_hltv_scraper import RealHLTVScraper
from datetime import datetime

def daily_scrape():
    """GÃ¼nlÃ¼k scraping fonksiyonu"""
    print(f"\nğŸ• {datetime.now()} - GÃ¼nlÃ¼k scraping baÅŸlÄ±yor...\n")
    
    scraper = RealHLTVScraper(headless=True)
    
    try:
        scraper.setup_driver()
        
        # GeÃ§miÅŸ maÃ§larÄ± gÃ¼ncelle (son 1 sayfa = son 50 maÃ§)
        results = scraper.scrape_results(num_pages=1)
        
        # Mevcut veriyle birleÅŸtir
        try:
            existing = pd.read_csv('hltv_match_results.csv')
            combined = pd.concat([existing, results]).drop_duplicates(
                subset=['team_1', 'team_2', 'match_date'], 
                keep='last'
            )
            combined.to_csv('hltv_match_results.csv', index=False)
        except:
            results.to_csv('hltv_match_results.csv', index=False)
        
        # Gelecek maÃ§larÄ± gÃ¼ncelle
        upcoming = scraper.scrape_upcoming_matches()
        upcoming.to_csv('hltv_upcoming_matches.csv', index=False)
        
        print(f"âœ… GÃ¼nlÃ¼k scraping tamamlandÄ±!")
        
    finally:
        scraper.close()

# Her gÃ¼n 09:00'da Ã§alÄ±ÅŸtÄ±r
schedule.every().day.at("09:00").do(daily_scrape)

print("â° GÃ¼nlÃ¼k scraper baÅŸlatÄ±ldÄ± (09:00)")
print("   Durdurmak iÃ§in: Ctrl+C")

while True:
    schedule.run_pending()
    time.sleep(60)
```

Ã‡alÄ±ÅŸtÄ±r:
```bash
python daily_scraper.py
```

### 4. Proxy KullanÄ±mÄ± (IP Ban Ã–nleme)

```python
from selenium.webdriver.common.proxy import Proxy, ProxyType

def setup_driver_with_proxy(self, proxy_address):
    """Proxy ile driver baÅŸlat"""
    chrome_options = Options()
    
    # Proxy ayarla
    chrome_options.add_argument(f'--proxy-server={proxy_address}')
    
    # ... diÄŸer ayarlar ...
    
    self.driver = webdriver.Chrome(service=service, options=chrome_options)

# KullanÄ±m
scraper = RealHLTVScraper()
scraper.setup_driver_with_proxy('http://proxy-server:port')
```

### 5. Hata ToleransÄ± ve Yeniden Deneme

```python
def scrape_with_retry(self, max_retries=3):
    """Hata durumunda yeniden dene"""
    for attempt in range(max_retries):
        try:
            results = self.scrape_results()
            return results
        except Exception as e:
            logger.warning(f"Deneme {attempt + 1}/{max_retries} baÅŸarÄ±sÄ±z: {e}")
            if attempt < max_retries - 1:
                time.sleep(5)  # 5 saniye bekle
            else:
                raise
```

---

## ğŸ› SORUN GÄ°DERME

### Hata 1: "ChromeDriver executable not found"

**Ã‡Ã¶zÃ¼m:**
```bash
# Otomatik Ã§Ã¶zÃ¼m (webdriver-manager kullanÄ±yor)
pip install webdriver-manager --upgrade

# Manuel Ã§Ã¶zÃ¼m
# ChromeDriver'Ä± indirin: https://chromedriver.chromium.org/
# PATH'e ekleyin veya kodda belirtin
```

### Hata 2: "Element not found" / "NoSuchElementException"

**Sebep:** HLTV HTML yapÄ±sÄ± deÄŸiÅŸmiÅŸ olabilir

**Ã‡Ã¶zÃ¼m:**
```python
# 1. TarayÄ±cÄ±yÄ± gÃ¶rÃ¼nÃ¼r modda aÃ§Ä±n
scraper = RealHLTVScraper(headless=False)

# 2. SayfayÄ± inspect edin
# 3. CSS selector'larÄ± gÃ¼ncelleyin

# Ã–rnek: Eski selector Ã§alÄ±ÅŸmÄ±yorsa
# Eski: ".result-con"
# Yeni: ".results-holder .result-con"
```

**GÃ¼ncel selector'larÄ± bulma:**
```python
# Chrome DevTools (F12) kullanÄ±n
# 1. Element'i seÃ§in
# 2. SaÄŸ tÄ±k â†’ Copy â†’ Copy selector
# 3. Kodda deÄŸiÅŸtirin
```

### Hata 3: "TimeoutException"

**Ã‡Ã¶zÃ¼m:**
```python
# Bekleme sÃ¼resini artÄ±rÄ±n
wait = WebDriverWait(self.driver, 20)  # 10'dan 20'ye

# Veya daha fazla bekleyin
time.sleep(5)  # 3'ten 5'e
```

### Hata 4: "403 Forbidden" / "Cloudflare"

**Sebep:** HLTV bot algÄ±lÄ±yor

**Ã‡Ã¶zÃ¼m:**
```python
# 1. User agent gÃ¼ncelle
chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")

# 2. Daha yavaÅŸ scrape et
time.sleep(random.randint(3, 7))  # Rastgele bekleme

# 3. Cookies kabul et
try:
    cookie_button = driver.find_element(By.ID, "cookie-accept")
    cookie_button.click()
except:
    pass
```

### Hata 5: Ã‡ok YavaÅŸ Ã‡alÄ±ÅŸÄ±yor

**Optimizasyon:**
```python
# 1. Sadece gerekli sayfalarÄ± yÃ¼kle
chrome_options.add_argument("--blink-settings=imagesEnabled=false")  # Resimleri devre dÄ±ÅŸÄ± bÄ±rak

# 2. CSS/JS yÃ¼klemesini engelle
prefs = {"profile.managed_default_content_settings.images": 2}
chrome_options.add_experimental_option("prefs", prefs)

# 3. Paralel scraping (dikkatli kullanÄ±n!)
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=3) as executor:
    results = list(executor.map(scrape_page, page_numbers))
```

---

## âš–ï¸ ETÄ°K VE YASAL

### âœ… Ä°ZÄ°N VERÄ°LEN KULLANIM

- ğŸ“š **AraÅŸtÄ±rma amaÃ§lÄ±**: Akademik veya kiÅŸisel projeler
- ğŸ“Š **Analiz**: TakÄ±m performans analizi
- ğŸ“ **EÄŸitim**: Ã–ÄŸrenme ve pratik yapma

### âŒ Ä°ZÄ°N VERÄ°LMEYEN KULLANIM

- ğŸš« **Ticari kullanÄ±m** olmadan izin
- ğŸš« **AÅŸÄ±rÄ± yÃ¼k** oluÅŸturma (DDoS benzeri)
- ğŸš« **Veri satÄ±ÅŸÄ±**
- ğŸš« **HLTV'ye rakip site** oluÅŸturma

### ğŸ“œ robots.txt KontrolÃ¼

```bash
# HLTV'nin robots.txt'ini kontrol edin
curl https://www.hltv.org/robots.txt
```

**robots.txt iÃ§eriÄŸine uyun!**

### ğŸ¤ Ä°yi Pratikler

1. **Rate Limiting**: Sayfa baÅŸÄ± en az 2-3 saniye bekle
   ```python
   time.sleep(random.uniform(2, 5))
   ```

2. **Makul KullanÄ±m**: GÃ¼nde en fazla 500-1000 maÃ§ Ã§ek

3. **Hata Durumunda Dur**: SÃ¼rekli hata alÄ±yorsanÄ±z Ã§ekmekten vazgeÃ§in

4. **Verileri Ã–nbellekle**: AynÄ± veriyi tekrar Ã§ekmeyin
   ```python
   # CSV'ye kaydet, sonra oradan oku
   if os.path.exists('cache.csv'):
       df = pd.read_csv('cache.csv')
   ```

5. **User-Agent Bildir**: KimliÄŸinizi belli edin
   ```python
   headers = {
       'User-Agent': 'Your-Bot-Name/1.0 (your-email@example.com)'
   }
   ```

---

## ğŸ“Š VERÄ° KALÄ°TESÄ°

### Kontrol Listesi

```python
def validate_data(df):
    """Ã‡ekilen veriyi doÄŸrula"""
    
    # 1. BoÅŸ deÄŸerler
    print("BoÅŸ deÄŸerler:")
    print(df.isnull().sum())
    
    # 2. Duplikatlar
    duplicates = df.duplicated().sum()
    print(f"\nDuplikat satÄ±rlar: {duplicates}")
    
    # 3. GeÃ§ersiz skorlar
    invalid_scores = df[(df['score_1'] < 0) | (df['score_2'] < 0)]
    print(f"GeÃ§ersiz skorlar: {len(invalid_scores)}")
    
    # 4. Tarih aralÄ±ÄŸÄ±
    df['match_date'] = pd.to_datetime(df['match_date'])
    print(f"\nTarih aralÄ±ÄŸÄ±: {df['match_date'].min()} - {df['match_date'].max()}")
    
    return df

# KullanÄ±m
results = scraper.scrape_results()
results = validate_data(results)
```

---

## ğŸ”„ GÃœNCELLEME STRATEJÄ°SÄ°

### Strateji 1: Tam GÃ¼ncelleme (HaftalÄ±k)

```bash
# TÃ¼m veriyi yeniden Ã§ek
python real_hltv_scraper.py --full
```

### Strateji 2: ArtÄ±mlÄ± GÃ¼ncelleme (GÃ¼nlÃ¼k)

```python
# Sadece son 1 gÃ¼nÃ¼n maÃ§larÄ±nÄ± Ã§ek
# Mevcut veriyle birleÅŸtir
```

### Strateji 3: Hibrit (Ã–nerilen)

```
- Her gÃ¼n: Son 1 sayfa (~50 maÃ§)
- Her hafta: Son 5 sayfa (~250 maÃ§) 
- Her ay: TÃ¼m veri (~1000+ maÃ§)
```

---

## ğŸ’¡ Ä°PUÃ‡LARI

1. **Ä°lk defa Ã§alÄ±ÅŸtÄ±rÄ±rken**:
   ```bash
   # Headless=False ile test edin
   # TarayÄ±cÄ±yÄ± gÃ¶rerek ne olduÄŸunu anlayÄ±n
   ```

2. **Selector bulamÄ±yorsanÄ±z**:
   - HLTV HTML'i deÄŸiÅŸmiÅŸ olabilir
   - Chrome DevTools ile yeni selector'larÄ± bulun
   - XPath kullanmayÄ± deneyin

3. **Veri Ã§ok bÃ¼yÃ¼kse**:
   ```python
   # Chunking kullanÄ±n
   for chunk in pd.read_csv('big_file.csv', chunksize=1000):
       process(chunk)
   ```

4. **Scraping engellenirse**:
   - VPN kullanÄ±n
   - Proxy servis kullanÄ±n (Ã¶rn: ScraperAPI, Bright Data)
   - Headless browser yerine gerÃ§ek browser kullanÄ±n

---

## ğŸ“ YARDIM

Sorun yaÅŸÄ±yorsanÄ±z:

1. **Log'larÄ± kontrol edin**:
   ```bash
   python real_hltv_scraper.py 2>&1 | tee scraper.log
   ```

2. **Verbose mode aÃ§Ä±n**:
   ```python
   logging.basicConfig(level=logging.DEBUG)
   ```

3. **Screenshot alÄ±n**:
   ```python
   driver.save_screenshot('error.png')
   ```

---

## âœ… Ã–ZET: HIZLI BAÅLANGIÃ‡

```bash
# 1. Kurulum
pip install selenium webdriver-manager pandas

# 2. Scraper'Ä± Ã§alÄ±ÅŸtÄ±r
python real_hltv_scraper.py

# 3. Verileri kontrol et
ls -lh hltv_*.csv

# 4. Modelleri eÄŸit
python precise_predictor.py

# 5. BaÅŸarÄ±! ğŸ‰
```

---

**Not**: Bu rehber HLTV.org'un mevcut (2026) yapÄ±sÄ±na gÃ¶re hazÄ±rlanmÄ±ÅŸtÄ±r. Site yapÄ±sÄ± deÄŸiÅŸirse scraper gÃ¼ncellenmesi gerekebilir.
